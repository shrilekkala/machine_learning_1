{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/vveitch/causality-tutorials/blob/main/ATE_Estimation_with_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QfZkNLUb4B-p"
   },
   "source": [
    "# ATT Estimation Tutorial\n",
    "\n",
    "This tutorial gives a short example for how to estimate average treatment effect on the treated using machine learning methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dS2X3Bq1-fxE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "import sklearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nxJ46X9cFJ9X"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED=42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPbJeayiEs3u"
   },
   "source": [
    "## Load and Format LaLonde Observational Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2AC9TPko-hbt"
   },
   "outputs": [],
   "source": [
    "def make_data_lalonde(df):\n",
    "    df_new = df.drop(['nodegree'], axis=1)\n",
    "    df_new['pos74'] = (df_new['RE74'] > 0).astype(int)\n",
    "    df_new['pos75'] = (df_new['RE75'] > 0).astype(int)\n",
    "    df_new['treatment'] = df_new['treatment'].astype(int)\n",
    "    return df_new\n",
    "\n",
    "\n",
    "col_names = ['treatment', 'age', 'education', 'black',\n",
    "             'hispanic', 'married', 'nodegree', 'RE74', 'RE75', 'RE78']\n",
    "control = pd.read_csv('https://raw.githubusercontent.com/anishazaveri/austen_plots/master/data/imbens-raw/psid_controls.txt', header=None, sep=r\"\\s\\s\", names=col_names, engine='python')\n",
    "treatment = pd.read_csv('https://raw.githubusercontent.com/anishazaveri/austen_plots/master/data/imbens-raw/nswre74_treated.txt', header=None, sep=r\"\\s\\s\", names=col_names, engine='python')\n",
    "\n",
    "lalonde1 = pd.concat([control, treatment]).reset_index(drop=True)\n",
    "lalonde1 = make_data_lalonde(lalonde1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "-A1LX6-t-hZD",
    "outputId": "b0e276e2-dce3-424d-ffc7-e2b992ad62ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>treatment</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>black</th>\n",
       "      <th>hispanic</th>\n",
       "      <th>married</th>\n",
       "      <th>RE74</th>\n",
       "      <th>RE75</th>\n",
       "      <th>RE78</th>\n",
       "      <th>pos74</th>\n",
       "      <th>pos75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   treatment   age  education  black  hispanic  married  RE74  RE75  RE78  \\\n",
       "0          0  47.0       12.0    0.0       0.0      0.0   0.0   0.0   0.0   \n",
       "1          0  50.0       12.0    1.0       0.0      1.0   0.0   0.0   0.0   \n",
       "2          0  44.0       12.0    0.0       0.0      0.0   0.0   0.0   0.0   \n",
       "3          0  28.0       12.0    1.0       0.0      1.0   0.0   0.0   0.0   \n",
       "4          0  54.0       12.0    0.0       0.0      1.0   0.0   0.0   0.0   \n",
       "\n",
       "   pos74  pos75  \n",
       "0      0      0  \n",
       "1      0      0  \n",
       "2      0      0  \n",
       "3      0      0  \n",
       "4      0      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lalonde1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "APOqpHmrOGzo"
   },
   "outputs": [],
   "source": [
    "confounders = lalonde1.drop(columns=['RE78', 'treatment'])\n",
    "outcome = lalonde1['RE78']\n",
    "treatment = lalonde1['treatment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C576dWRsa3ad"
   },
   "source": [
    "## Specify Nuisance Function Models\n",
    "\n",
    "The next step is to specify models for the conditional expected outcome and propensity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qyOhSZRQRb8W",
    "outputId": "7df1d854-c13f-4f93-ec7c-4977df6ad283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE of fit model 105818236.45816422\n",
      "Test MSE of no-covariate model 246319790.55062827\n"
     ]
    }
   ],
   "source": [
    "# specify a model for the conditional expected outcome\n",
    "\n",
    "# make a function that returns a sklearn model for later use in k-folding\n",
    "def make_Q_model():\n",
    "  return RandomForestRegressor(random_state=RANDOM_SEED, n_estimators=500, max_depth=None)\n",
    "Q_model = make_Q_model()\n",
    "\n",
    "# Sanity check that chosen model actually improves test error\n",
    "# A real analysis should give substantial attention to model selection and validation \n",
    "\n",
    "X_w_treatment = confounders.copy()\n",
    "X_w_treatment[\"treatment\"] = treatment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_w_treatment, outcome, test_size=0.2)\n",
    "Q_model.fit(X_train, y_train)\n",
    "y_pred = Q_model.predict(X_test)\n",
    "\n",
    "test_mse=mean_squared_error(y_pred, y_test)\n",
    "print(f\"Test MSE of fit model {test_mse}\") \n",
    "baseline_mse=mean_squared_error(y_train.mean()*np.ones_like(y_test), y_test)\n",
    "print(f\"Test MSE of no-covariate model {baseline_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uq6eZEBXbsaI",
    "outputId": "974c356c-07f3-4573-f8c3-b83400c82169"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CE of fit model 0.07789407933364972\n",
      "Test CE of no-covariate model 0.21817471356014154\n"
     ]
    }
   ],
   "source": [
    "# specify a model for the propensity score\n",
    "\n",
    "def make_g_model():\n",
    "#  return LogisticRegression(max_iter=1000)\n",
    "  return RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "\n",
    "g_model = make_g_model()\n",
    "# Sanity check that chosen model actually improves test error\n",
    "# A real analysis should give substantial attention to model selection and validation \n",
    "\n",
    "X_train, X_test, a_train, a_test = train_test_split(confounders, treatment, test_size=0.2)\n",
    "g_model.fit(X_train, a_train)\n",
    "a_pred = g_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "test_ce=log_loss(a_test, a_pred)\n",
    "print(f\"Test CE of fit model {test_ce}\") \n",
    "baseline_ce=log_loss(a_test, a_train.mean()*np.ones_like(a_test))\n",
    "print(f\"Test CE of no-covariate model {baseline_ce}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RkvV_4_dFWo"
   },
   "source": [
    "## Use cross fitting to get get predicted outcomes and propensity scores for each unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KA0AsEGJ_X3b"
   },
   "outputs": [],
   "source": [
    "# helper functions to implement the cross fitting\n",
    "\n",
    "def treatment_k_fold_fit_and_predict(make_model, X:pd.DataFrame, A:np.array, n_splits:int):\n",
    "    \"\"\"\n",
    "    Implements K fold cross-fitting for the model predicting the treatment A. \n",
    "    That is, \n",
    "    1. Split data into K folds\n",
    "    2. For each fold j, the model is fit on the other K-1 folds\n",
    "    3. The fitted model is used to make predictions for each data point in fold j\n",
    "    Returns an array containing the predictions  \n",
    "\n",
    "    Args:\n",
    "    model: function that returns sklearn model (which implements fit and predict_prob)\n",
    "    X: dataframe of variables to adjust for\n",
    "    A: array of treatments\n",
    "    n_splits: number of splits to use\n",
    "    \"\"\"\n",
    "    predictions = np.full_like(A, np.nan, dtype=float)\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, A):\n",
    "      X_train = X.loc[train_index]\n",
    "      A_train = A.loc[train_index]\n",
    "      g = make_model()\n",
    "      g.fit(X_train, A_train)\n",
    "\n",
    "      # get predictions for split\n",
    "      predictions[test_index] = g.predict_proba(X.loc[test_index])[:, 1]\n",
    "\n",
    "    assert np.isnan(predictions).sum() == 0\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def outcome_k_fold_fit_and_predict(make_model, X:pd.DataFrame, y:np.array, A:np.array, n_splits:int, output_type:str):\n",
    "    \"\"\"\n",
    "    Implements K fold cross-fitting for the model predicting the outcome Y. \n",
    "    That is, \n",
    "    1. Split data into K folds\n",
    "    2. For each fold j, the model is fit on the other K-1 folds\n",
    "    3. The fitted model is used to make predictions for each data point in fold j\n",
    "    Returns two arrays containing the predictions for all units untreated, all units treated  \n",
    "\n",
    "    Args:\n",
    "    model: function that returns sklearn model (that implements fit and either predict_prob or predict)\n",
    "    X: dataframe of variables to adjust for\n",
    "    y: array of outcomes\n",
    "    A: array of treatments\n",
    "    n_splits: number of splits to use\n",
    "    output_type: type of outcome, \"binary\" or \"continuous\"\n",
    "\n",
    "    \"\"\"\n",
    "    predictions0 = np.full_like(A, np.nan, dtype=float)\n",
    "    predictions1 = np.full_like(y, np.nan, dtype=float)\n",
    "    if output_type == 'binary':\n",
    "      kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    elif output_type == 'continuous':\n",
    "      kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    # include the treatment as input feature\n",
    "    X_w_treatment = X.copy()\n",
    "    X_w_treatment[\"A\"] = A\n",
    "\n",
    "    # for predicting effect under treatment / control status for each data point \n",
    "    X0 = X_w_treatment.copy()\n",
    "    X0[\"A\"] = 0\n",
    "    X1 = X_w_treatment.copy()\n",
    "    X1[\"A\"] = 1\n",
    "\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_w_treatment, y):\n",
    "      X_train = X_w_treatment.loc[train_index]\n",
    "      y_train = y.loc[train_index]\n",
    "      q = make_model()\n",
    "      q.fit(X_train, y_train)\n",
    "\n",
    "      if output_type =='binary':\n",
    "        predictions0[test_index] = q.predict_proba(X0.loc[test_index])[:, 1]\n",
    "        predictions1[test_index] = q.predict_proba(X1.loc[test_index])[:, 1]\n",
    "      elif output_type == 'continuous':\n",
    "        predictions0[test_index] = q.predict(X0.loc[test_index])\n",
    "        predictions1[test_index] = q.predict(X1.loc[test_index])\n",
    "\n",
    "    assert np.isnan(predictions0).sum() == 0\n",
    "    assert np.isnan(predictions1).sum() == 0\n",
    "    return predictions0, predictions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wVcE6pRQeMNf"
   },
   "outputs": [],
   "source": [
    "g = treatment_k_fold_fit_and_predict(make_g_model, X=confounders, A=treatment, n_splits=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GLEHlLLdWSh9"
   },
   "outputs": [],
   "source": [
    "Q0,Q1=outcome_k_fold_fit_and_predict(make_Q_model, X=confounders, y=outcome, A=treatment, n_splits=10, output_type=\"continuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "id": "_NVCV0q0g8wQ",
    "outputId": "b638a74f-1c3f-4860-bd3d-af9eb397832a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g</th>\n",
       "      <th>Q0</th>\n",
       "      <th>Q1</th>\n",
       "      <th>A</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.313350</td>\n",
       "      <td>91.666309</td>\n",
       "      <td>1612.832142</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.191958</td>\n",
       "      <td>2017.511419</td>\n",
       "      <td>3896.496922</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.470788</td>\n",
       "      <td>43.925636</td>\n",
       "      <td>1767.068998</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.517957</td>\n",
       "      <td>11169.331018</td>\n",
       "      <td>9051.689591</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014246</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2117.226099</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          g            Q0           Q1  A    Y\n",
       "0  0.313350     91.666309  1612.832142  0  0.0\n",
       "1  0.191958   2017.511419  3896.496922  0  0.0\n",
       "2  0.470788     43.925636  1767.068998  0  0.0\n",
       "3  0.517957  11169.331018  9051.689591  0  0.0\n",
       "4  0.014246      0.000000  2117.226099  0  0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_and_nuisance_estimates = pd.DataFrame({'g': g, 'Q0': Q0, 'Q1': Q1, 'A': treatment, 'Y': outcome})\n",
    "data_and_nuisance_estimates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNhM7URdgzQB"
   },
   "source": [
    "## Combine predicted values and data into estimate of ATT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "J-vONC5ejwh2"
   },
   "outputs": [],
   "source": [
    "def att_aiptw(Q0, Q1, g, A, Y, prob_t=None):\n",
    "  \"\"\"\n",
    "  # Double ML estimator for the ATT\n",
    "  This uses the ATT specific scores, see equation 3.9 of https://www.econstor.eu/bitstream/10419/149795/1/869216953.pdf\n",
    "  \"\"\"\n",
    "\n",
    "  if prob_t is None:\n",
    "    prob_t = A.mean() # estimate marginal probability of treatment\n",
    "\n",
    "  tau_hat = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0)).mean()/ prob_t\n",
    "  \n",
    "  scores = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0) - tau_hat*A) / prob_t\n",
    "  n = Y.shape[0] # number of observations\n",
    "  std_hat = np.std(scores) / np.sqrt(n)\n",
    "\n",
    "  return tau_hat, std_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "O_F5r0SSkzzK"
   },
   "outputs": [],
   "source": [
    "def ate_aiptw(Q0, Q1, g, A, Y, prob_t=None):\n",
    "  \"\"\"\n",
    "  # Double ML estimator for the ATE\n",
    "  \"\"\"\n",
    "\n",
    "  tau_hat = (Q1 - Q0 + A*(Y-Q1)/g - (1-A)*(Y-Q0)/(1-g)).mean()\n",
    "  \n",
    "  scores = Q1 - Q0 + A*(Y-Q1)/g - (1-A)*(Y-Q0)/(1-g) - tau_hat\n",
    "  n = Y.shape[0] # number of observations\n",
    "  std_hat = np.std(scores) / np.sqrt(n)\n",
    "\n",
    "  return tau_hat, std_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SjDj0F9Bm9uq",
    "outputId": "fdef5c08-3829-400b-ea0e-1cb5dd01bc25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate is 1295.9085019166787 pm 1624.0222463376508\n"
     ]
    }
   ],
   "source": [
    "tau_hat, std_hat = att_aiptw(**data_and_nuisance_estimates)\n",
    "print(f\"The estimate is {tau_hat} pm {1.96*std_hat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSaOp1HwlQ4i",
    "outputId": "874e2ea0-dfc1-4594-9d45-b6663f69f163"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate is -34006.316596350116 pm 51141.073469511226\n"
     ]
    }
   ],
   "source": [
    "in_treated = data_and_nuisance_estimates['A']==1\n",
    "treated_estimates = data_and_nuisance_estimates[in_treated]\n",
    "tau_hat, std_hat = ate_aiptw(**treated_estimates)\n",
    "print(f\"The estimate is {tau_hat} pm {1.96*std_hat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IOuJnlbEo8j_",
    "outputId": "74678791-7163-41e6-f7a9-a04a7b669e81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate is 563.251470303294 pm 1502.651264797957\n"
     ]
    }
   ],
   "source": [
    "# The LaLonde data has severe overlap issues. Lets try computing the estimate restricted to a population with only reasonable propensity scores\n",
    "g = data_and_nuisance_estimates['g']\n",
    "in_overlap_popluation = (g < 0.90)\n",
    "overlap_data_and_nuisance = data_and_nuisance_estimates[in_overlap_popluation]\n",
    "tau_hat, std_hat = att_aiptw(**overlap_data_and_nuisance)\n",
    "print(f\"The estimate is {tau_hat} pm {1.96*std_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def att_plug_in(Q0, Q1, Y):\n",
    "  \"\"\"\n",
    "  Simple plug-in estimator for the ATT\n",
    "\n",
    "  args:\n",
    "  Q0: predictions for treated units conditional on A=0 (untreated)\n",
    "  Q1: predictions for treated units conditional on A=1 (treated)\n",
    "  Y: array of outcomes for treated units\n",
    "  \"\"\"\n",
    "  # compute the mean of F(1, X) - F(0, X) for only those individuals who received treatment\n",
    "  tau_hat = (Q1 - Q0).mean()\n",
    "  \n",
    "  scores = (Q1 - Q0) - tau_hat\n",
    "  n = Y.shape[0] # number of observations\n",
    "  std_hat = np.std(scores) / np.sqrt(n)\n",
    "\n",
    "  return tau_hat, std_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate of τ^(ATT) with the plug in estimator is 1085.3404537748204\n",
      "The estimate of τ^(ATT) with the advanced ML approach is 563.251470303294\n"
     ]
    }
   ],
   "source": [
    "# Only choose samples who received the treatment\n",
    "in_treated = data_and_nuisance_estimates['A'] == 1\n",
    "treated_estimates = data_and_nuisance_estimates[in_treated][[\"Q0\", \"Q1\", \"Y\"]]\n",
    "\n",
    "# Pass the samples and their predictions to the ATT function\n",
    "tau_ATT, std_ATT = att_plug_in(**treated_estimates)\n",
    "\n",
    "print(f\"The estimate of τ^(ATT) with the plug in estimator is {tau_ATT}\")\n",
    "\n",
    "# Compare with the estimator from above with the \"reasonable\" propensity score\n",
    "print(f\"The estimate of τ^(ATT) with the advanced ML approach is {tau_hat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that our estimator is not very close to the advanced estimator, being almost twice the quantity of the latter.\n",
    "\n",
    "However this may be explained by the fact that the plug in estimator does not take into account the effect of the nuisance parameters specified in the model as well as the propensity scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for estimate of τ^(ATT) using the bootstrap: [669.2862120150093, 1506.8020959546695]\n",
      "95% Confidence Interval for estimate of τ^(ATT) with the advanced ML approach: [-939.3997944946631, 2065.902735101251]\n"
     ]
    }
   ],
   "source": [
    "# Number of bootstrap samples\n",
    "num_bootstrap_samples = 1000\n",
    "\n",
    "# Function to compute τ^{ATT} for each bootstrap sample (with replacement)\n",
    "def compute_tau_att_bootstrap_sample(data_and_nuisance_estimates):\n",
    "    bootstrap_sample = data_and_nuisance_estimates.sample(frac=1, replace=True)\n",
    "\n",
    "    in_treated = bootstrap_sample['A'] == 1\n",
    "    treated_estimates = bootstrap_sample[in_treated][[\"Q0\", \"Q1\", \"Y\"]]\n",
    "    \n",
    "    tau_ATT, _ = att_plug_in(**treated_estimates)\n",
    "    return tau_ATT\n",
    "\n",
    "# Perform bootstrap\n",
    "bootstrap_samples = [compute_tau_att_bootstrap_sample(data_and_nuisance_estimates) for _ in range(num_bootstrap_samples)]\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_samples, [2.5, 97.5])\n",
    "print(f\"95% Confidence Interval for estimate of τ^(ATT) using the bootstrap: {list(confidence_interval)}\")\n",
    "\n",
    "# Compare against the confidence interval for the estimator from above with the \"reasonable\" propensity score\n",
    "print(f\"95% Confidence Interval for estimate of τ^(ATT) with the advanced ML approach: {[tau_hat - std_hat * 1.96, tau_hat + std_hat * 1.96]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again we observe that the confidence intervals for the two approaches differ, and this can be explained for the same reasons as in 3 c).  \n",
    "However, one can note that the confidence interval for the bootstrap estimator is wholly contained within the confidence interval for the more advanced approach that takes into account more factors than our simple estimator."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ATE-Estimation-with-Machine-Learning.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
